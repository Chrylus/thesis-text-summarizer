{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Install Needed Library"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:15:21.310170Z","iopub.status.busy":"2024-04-12T07:15:21.309856Z","iopub.status.idle":"2024-04-12T07:16:01.504740Z","shell.execute_reply":"2024-04-12T07:16:01.503613Z","shell.execute_reply.started":"2024-04-12T07:15:21.310127Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in c:\\users\\sanja\\anaconda3\\lib\\site-packages (4.39.3)\n","Requirement already satisfied: filelock in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n","Requirement already satisfied: requests in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: colorama in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: bert-extractive-summarizer in c:\\users\\sanja\\anaconda3\\lib\\site-packages (0.10.1)\n","Requirement already satisfied: transformers in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from bert-extractive-summarizer) (4.39.3)\n","Requirement already satisfied: scikit-learn in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from bert-extractive-summarizer) (1.2.2)\n","Requirement already satisfied: spacy in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from bert-extractive-summarizer) (3.7.4)\n","Requirement already satisfied: numpy>=1.17.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (2.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.9.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (5.2.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (4.65.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.10.12)\n","Requirement already satisfied: jinja2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.1.3)\n","Requirement already satisfied: setuptools in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (68.2.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n","Requirement already satisfied: filelock in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.22.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2023.10.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.4.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->bert-extractive-summarizer) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->bert-extractive-summarizer) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.1.4)\n","Requirement already satisfied: colorama in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->bert-extractive-summarizer) (0.4.6)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.3)\n"]}],"source":["!pip install transformers\n","!pip install bert-extractive-summarizer"]},{"cell_type":"markdown","metadata":{},"source":["## Coding"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-12T07:16:01.507451Z","iopub.status.busy":"2024-04-12T07:16:01.507135Z","iopub.status.idle":"2024-04-12T07:16:01.772132Z","shell.execute_reply":"2024-04-12T07:16:01.771179Z","shell.execute_reply.started":"2024-04-12T07:16:01.507384Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from numpy import *\n","import warnings\n","import multiprocessing\n","from matplotlib import pyplot\n","import gensim\n","import re\n","from bs4 import BeautifulSoup\n","\n","from sklearn.model_selection import train_test_split\n","\n","from keras import backend as K \n","\n","from nltk.corpus import stopwords\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["max_text_len=900\n","max_summary_len=100"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["post_pre = pd.read_csv ('..\\Dataset\\Indonesian News Dataset Preprocessed.csv', sep=';')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:40.555778Z","iopub.status.busy":"2024-04-12T07:16:40.555540Z","iopub.status.idle":"2024-04-12T07:16:40.566167Z","shell.execute_reply":"2024-04-12T07:16:40.565515Z","shell.execute_reply.started":"2024-04-12T07:16:40.555738Z"},"trusted":true},"outputs":[],"source":["#Add sostok and eostok at \n","post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:40.567916Z","iopub.status.busy":"2024-04-12T07:16:40.567631Z","iopub.status.idle":"2024-04-12T07:16:40.584265Z","shell.execute_reply":"2024-04-12T07:16:40.583396Z","shell.execute_reply.started":"2024-04-12T07:16:40.567872Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>jakarta anggota komisi vii dpr ri rofik hanan...</td>\n","      <td>sostok _START_ anggota komisi vii dpr ri rofik...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>jakarta presiden joko widodo atau jokowi meme...</td>\n","      <td>sostok _START_ presiden joko widodo telah meme...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>wakil ketua mpr ri dr hidayat nur wahid ma at...</td>\n","      <td>sostok _START_ wakil ketua mpr ri dr hidayat n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jakarta tim kedokteran dan kesehatan dokkes p...</td>\n","      <td>sostok _START_ tim kedokteran dan kesehatan do...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ketua mpr ri sekaligus ketua umum ikatan moto...</td>\n","      <td>sostok _START_ ketua mpr ri bambang soesatyo t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0   jakarta anggota komisi vii dpr ri rofik hanan...   \n","1   jakarta presiden joko widodo atau jokowi meme...   \n","2   wakil ketua mpr ri dr hidayat nur wahid ma at...   \n","3   jakarta tim kedokteran dan kesehatan dokkes p...   \n","4   ketua mpr ri sekaligus ketua umum ikatan moto...   \n","\n","                                             summary  \n","0  sostok _START_ anggota komisi vii dpr ri rofik...  \n","1  sostok _START_ presiden joko widodo telah meme...  \n","2  sostok _START_ wakil ketua mpr ri dr hidayat n...  \n","3  sostok _START_ tim kedokteran dan kesehatan do...  \n","4  sostok _START_ ketua mpr ri bambang soesatyo t...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["post_pre = post_pre[post_pre['summary'].notna()]\n","post_pre = post_pre[post_pre['text'].notna()]\n","\n","post_pre.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:40.586000Z","iopub.status.busy":"2024-04-12T07:16:40.585700Z","iopub.status.idle":"2024-04-12T07:16:41.202737Z","shell.execute_reply":"2024-04-12T07:16:41.201554Z","shell.execute_reply.started":"2024-04-12T07:16:40.585944Z"},"trusted":true},"outputs":[],"source":["x_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.1,random_state=0,shuffle=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:41.204939Z","iopub.status.busy":"2024-04-12T07:16:41.204591Z","iopub.status.idle":"2024-04-12T07:16:43.238093Z","shell.execute_reply":"2024-04-12T07:16:43.237023Z","shell.execute_reply.started":"2024-04-12T07:16:41.204877Z"},"trusted":true},"outputs":[],"source":["#Lets tokenize the text to get the vocab count , you can use Spacy here also\n","\n","#prepare a tokenizer for reviews on training data\n","x_tokenizer = Tokenizer() \n","x_tokenizer.fit_on_texts(list(x_tr))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:43.239939Z","iopub.status.busy":"2024-04-12T07:16:43.239620Z","iopub.status.idle":"2024-04-12T07:16:43.269179Z","shell.execute_reply":"2024-04-12T07:16:43.268264Z","shell.execute_reply.started":"2024-04-12T07:16:43.239883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["% of rare words in vocabulary: 59.88730392405706\n","Total Coverage of rare words: 1.224444203282535\n"]}],"source":["thresh=4\n","\n","cnt=0\n","tot_cnt=0\n","freq=0\n","tot_freq=0\n","\n","for key,value in x_tokenizer.word_counts.items():\n","    tot_cnt=tot_cnt+1\n","    tot_freq=tot_freq+value\n","    if(value<thresh):\n","        cnt=cnt+1\n","        freq=freq+value\n","    \n","print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n","print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:43.271018Z","iopub.status.busy":"2024-04-12T07:16:43.270691Z","iopub.status.idle":"2024-04-12T07:16:43.928374Z","shell.execute_reply":"2024-04-12T07:16:43.927242Z","shell.execute_reply.started":"2024-04-12T07:16:43.270961Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of vocabulary in X = 39510\n"]}],"source":["#prepare a tokenizer for reviews on training data\n","x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n","x_tokenizer.fit_on_texts(list(x_tr))\n","\n","#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n","x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n","x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n","\n","#padding zero upto maximum length\n","x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n","x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n","\n","#size of vocabulary ( +1 for padding token)\n","x_voc   =  x_tokenizer.num_words + 1\n","\n","print(\"Size of vocabulary in X = {}\".format(x_voc))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:43.930210Z","iopub.status.busy":"2024-04-12T07:16:43.929906Z","iopub.status.idle":"2024-04-12T07:16:44.019497Z","shell.execute_reply":"2024-04-12T07:16:44.018577Z","shell.execute_reply.started":"2024-04-12T07:16:43.930151Z"},"trusted":true},"outputs":[],"source":["#prepare a tokenizer for reviews on training data\n","y_tokenizer = Tokenizer()   \n","y_tokenizer.fit_on_texts(list(y_tr))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:44.021158Z","iopub.status.busy":"2024-04-12T07:16:44.020883Z","iopub.status.idle":"2024-04-12T07:16:44.034005Z","shell.execute_reply":"2024-04-12T07:16:44.032600Z","shell.execute_reply.started":"2024-04-12T07:16:44.021107Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["% of rare words in vocabulary: 67.75484617610185\n","Total Coverage of rare words: 3.3650382902870914\n"]}],"source":["thresh=6\n","\n","cnt=0\n","tot_cnt=0\n","freq=0\n","tot_freq=0\n","\n","for key,value in y_tokenizer.word_counts.items():\n","    tot_cnt=tot_cnt+1\n","    tot_freq=tot_freq+value\n","    if(value<thresh):\n","        cnt=cnt+1\n","        freq=freq+value\n","    \n","print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n","print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:44.036158Z","iopub.status.busy":"2024-04-12T07:16:44.035691Z","iopub.status.idle":"2024-04-12T07:16:44.210209Z","shell.execute_reply":"2024-04-12T07:16:44.209161Z","shell.execute_reply.started":"2024-04-12T07:16:44.035969Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of vocabulary in Y = 13375\n"]}],"source":["#prepare a tokenizer for reviews on training data\n","y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n","y_tokenizer.fit_on_texts(list(y_tr))\n","\n","#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n","y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n","y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n","\n","#padding zero upto maximum length\n","y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n","y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n","\n","#size of vocabulary\n","y_voc  =   y_tokenizer.num_words +1\n","print(\"Size of vocabulary in Y = {}\".format(y_voc))"]},{"cell_type":"markdown","metadata":{},"source":["We will now remove \"Summary\" i.e Y (both train and val) which has only _START_ and _END_"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:44.212174Z","iopub.status.busy":"2024-04-12T07:16:44.211858Z","iopub.status.idle":"2024-04-12T07:16:44.515018Z","shell.execute_reply":"2024-04-12T07:16:44.513998Z","shell.execute_reply.started":"2024-04-12T07:16:44.212118Z"},"trusted":true},"outputs":[],"source":["ind=[]\n","for i in range(len(y_tr)):\n","    cnt=0\n","    for j in y_tr[i]:\n","        if j!=0:\n","            cnt=cnt+1\n","    if(cnt==2):\n","        ind.append(i)\n","\n","y_tr=np.delete(y_tr,ind, axis=0)\n","x_tr=np.delete(x_tr,ind, axis=0)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:44.516939Z","iopub.status.busy":"2024-04-12T07:16:44.516662Z","iopub.status.idle":"2024-04-12T07:16:44.560275Z","shell.execute_reply":"2024-04-12T07:16:44.559469Z","shell.execute_reply.started":"2024-04-12T07:16:44.516893Z"},"trusted":true},"outputs":[],"source":["ind=[]\n","for i in range(len(y_val)):\n","    cnt=0\n","    for j in y_val[i]:\n","        if j!=0:\n","            cnt=cnt+1\n","    if(cnt==2):\n","        ind.append(i)\n","\n","y_val=np.delete(y_val,ind, axis=0)\n","x_val=np.delete(x_val,ind, axis=0)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:16:44.562350Z","iopub.status.busy":"2024-04-12T07:16:44.561948Z","iopub.status.idle":"2024-04-12T07:16:44.573588Z","shell.execute_reply":"2024-04-12T07:16:44.572481Z","shell.execute_reply.started":"2024-04-12T07:16:44.562277Z"},"trusted":true},"outputs":[],"source":["def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n","    \"\"\"\n","    Encoder for encoding the text into sequence of integers for BERT Input\n","    \"\"\"\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding(max_length=maxlen)\n","    all_ids = []\n","    \n","    for i in tqdm(range(0, len(texts), chunk_size)):\n","        text_chunk = texts[i:i+chunk_size].tolist()\n","        encs = tokenizer.encode_batch(text_chunk)\n","        all_ids.extend([enc.ids for enc in encs])\n","    \n","    return np.array(all_ids)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:34.670556Z","iopub.status.busy":"2024-04-12T07:17:34.670212Z","iopub.status.idle":"2024-04-12T07:17:35.361906Z","shell.execute_reply":"2024-04-12T07:17:35.361099Z","shell.execute_reply.started":"2024-04-12T07:17:34.670503Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["REPLICAS:  1\n"]}],"source":["import tensorflow as tf\n","import transformers\n","\n","# Detect hardware, return appropriate distribution strategy\n","try:\n","    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n","    # set: this is always the case on Kaggle.\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","    strategy = tf.distribute.get_strategy()\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:35.364766Z","iopub.status.busy":"2024-04-12T07:17:35.364363Z","iopub.status.idle":"2024-04-12T07:17:35.370201Z","shell.execute_reply":"2024-04-12T07:17:35.369158Z","shell.execute_reply.started":"2024-04-12T07:17:35.364692Z"},"trusted":true},"outputs":[],"source":["#IMP DATA FOR CONFIG\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","\n","# Configuration\n","EPOCHS = 3\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","MAX_LEN = 192"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:35.372295Z","iopub.status.busy":"2024-04-12T07:17:35.371927Z","iopub.status.idle":"2024-04-12T07:17:35.382853Z","shell.execute_reply":"2024-04-12T07:17:35.381953Z","shell.execute_reply.started":"2024-04-12T07:17:35.372230Z"},"trusted":true},"outputs":[],"source":["# import transformers\n","# from transformers import DistilBertTokenizer, DistilBertModel\n","# from tokenizers import BertWordPieceTokenizer\n","\n","# # First load the real tokenizer\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n","# # Save the loaded tokenizer locally\n","# tokenizer.save_pretrained('.')\n","# # Reload it with the huggingface tokenizers library\n","# fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n","# fast_tokenizer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:35.475950Z","iopub.status.busy":"2024-04-12T07:17:35.475598Z","iopub.status.idle":"2024-04-12T07:17:35.513550Z","shell.execute_reply":"2024-04-12T07:17:35.512816Z","shell.execute_reply.started":"2024-04-12T07:17:35.475885Z"},"trusted":true},"outputs":[],"source":["train_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_tr, y_tr))\n","    .repeat()\n","    .shuffle(2048)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","valid_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_val, y_val))\n","    .batch(BATCH_SIZE)\n","    .cache()\n","    .prefetch(AUTO)\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:36.512703Z","iopub.status.busy":"2024-04-12T07:17:36.512357Z","iopub.status.idle":"2024-04-12T07:17:36.521379Z","shell.execute_reply":"2024-04-12T07:17:36.520200Z","shell.execute_reply.started":"2024-04-12T07:17:36.512654Z"},"trusted":true},"outputs":[],"source":["def build_model(transformer, max_len=512):\n","    \"\"\"\n","    Function for training the BERT model\n","    \"\"\"\n","    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    sequence_output = transformer(input_word_ids)[0]\n","    cls_token = sequence_output[:, 0, :]\n","    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n","    \n","    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n","    model.compile(tf.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T07:17:37.036749Z","iopub.status.busy":"2024-04-12T07:17:37.036390Z","iopub.status.idle":"2024-04-12T07:17:37.070162Z","shell.execute_reply":"2024-04-12T07:17:37.069134Z","shell.execute_reply.started":"2024-04-12T07:17:37.036690Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From c:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"]},{"ename":"ValueError","evalue":"Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n  • input_ids=<KerasTensor shape=(None, 192), dtype=int32, sparse=None, name=input_word_ids>\n  • attention_mask=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m      2\u001b[0m     transformer_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      3\u001b[0m         transformers\u001b[38;5;241m.\u001b[39mTFDistilBertModel\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     )\n\u001b[1;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(transformer_layer, max_len\u001b[38;5;241m=\u001b[39mMAX_LEN)\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[1;32mIn[21], line 6\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(transformer, max_len)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mFunction for training the BERT model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m input_word_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_word_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m transformer(input_word_ids)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m sequence_output[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m      8\u001b[0m out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(cls_token)\n","File \u001b[1;32mc:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n","File \u001b[1;32mc:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:566\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m         output[main_input_name] \u001b[38;5;241m=\u001b[39m main_input\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(main_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n","\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n  • input_ids=<KerasTensor shape=(None, 192), dtype=int32, sparse=None, name=input_word_ids>\n  • attention_mask=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"]}],"source":["with strategy.scope():\n","    transformer_layer = (\n","        transformers.TFDistilBertModel\n","        .from_pretrained('distilbert-base-multilingual-cased')\n","    )\n","    model = build_model(transformer_layer, max_len=MAX_LEN)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1895,"sourceId":791838,"sourceType":"datasetVersion"},{"datasetId":4778259,"sourceId":8093150,"sourceType":"datasetVersion"},{"sourceId":51841083,"sourceType":"kernelVersion"}],"dockerImageVersionId":29662,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
