{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len=900\n",
    "max_summary_len=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pre = pd.read_csv ('..\\Dataset\\Indonesian News Dataset Preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sostok and eostok at \n",
    "post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "\n",
    "post_pre['text'] = post_pre['text'].astype(str)\n",
    "post_pre['summary'] = post_pre['summary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 61.07469711496557\n",
      "Total Coverage of rare words: 1.1015455904901386\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 44660\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 58.33826851790924\n",
      "Total Coverage of rare words: 2.078358999120164\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in Y = 18995\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffc18a89dad489bb3be6e79b3074701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/999M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanja\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sanja\\.cache\\huggingface\\hub\\models--cahya--bert2bert-indonesian-summarization. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "model = EncoderDecoderModel.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presiden joko widodo ( sby ) memerintahkan agar ada solusi dalam satu dua hari ini usai kebakaran di depo pertamina plumpang jakarta utara.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "ARTICLE_TO_SUMMARIZE = \"menteri bumn erick thohir penjabat gubernur dki jakarta heru budi hartono dan direktur utama pertamina nicke widyawati siang ini langsung menggelar rapat di depo plumpang jakarta utara setelah diperintah presiden joko widodo atau jokowi jokowi memerintahkan agar ada solusi dalam satu dua hari ini usai kebakaran di depo pertamina apakah fasilitas penyimpanan bensin ini dipindah atau warga di sekitar di relokasi ini mau rapat di sana ini rapat langsung kata erick usai menemani jokowi yang mengunjungi para pengungsi kebakaran depo pertamina di rptra rasela koja jakarta utara ahad maret 2023.jokowi datang bersama erick heru yang juga kepala sekretariat presiden dan nicke nicke juga membenarkan rapat digelar di plumpang bersama erick dan heru iya kata dia di lokasi.di lokasi ini jokowi telah memerintahkan erick heru dan pertamina mencari solusi usai kejadian kebakaran di depo pertamina plumpang jakarta utara jokowi memberi dua opsi depo pertamina dipindahkan atau warga yang tinggal direlokasi ini akan segera diputuskan sehari dua hari ini oleh pertamina gubernur dki sehingga solusinya menjadi jelas kata jokowi.ada dua opsijokowi belum bisa memastikan apakah nanti warga masih bisa tinggal di lokasi jika nanti yang akhirnya depo pertamina yang dipindah ini yang baru nanti dibicarakan ada pilihan-pilihan apakah deponya yang digeser apakah masyarakatnya yang digeser kalau digeser tanahnya di mana tapi harus segera ditemukan solusinya kata dia.jokowi menyebut depo pertamina plumpang ini merupakan zona bahaya tidak bisa lagi ditinggali tetapi harus ada solusinya bisa saja plumpang-nya digeser ke reklamasi atau penduduknya yang digeser ke relokasi mantan gubernur dki jakarta 2012-2014.jokowi tidak merinci lebih lanjut reklamasi yang dia maksud menurut dia solusi akan dibicarakan oleh pt pertamina dan heru tapi semuanya memang harus zona-zona berbahaya ini tidak hanya di sini saja harus diaudit harus dievaluasi semuanya karena menyangkut nyawa tadi saya sudah perintahkan semuanya ujarnya.jokowi menyebut seharusnya depo pertamina ini harusnya menjadi zona air seperti misalnya dibuat sungai yang memisahkan depo dan pemukiman tujuannya untuk melindungi objek vital yang ada di dalam depo karena barang-barang didalamnya barang-barang yang sangat bahaya untuk berdekatan dengan masyarakat apalagi dengan pemukiman penduduk kata dia.wapres usul depo dipindah ke pelabuhankebakaran terjadi jumat malam maret 2023 yang menewaskan 17 orang dan 600 lebih mengungsi sebelum jokowi wakil presiden ma ruf amin sudah lebih dulu mengunjungi pengungsi di plumpang.ma ruf mengusulkan agar depo pertamina di plumpang dipindah menjauhi permukiman penduduk pascaterbakar pada jumat malam kemarin kebakaran depo menjalar ke pemukiman warga hingga mengakibatkan jatuhnya korban jiwa saya berharap supaya depo ini lebih aman itu bisa direlokasi di pelabuhan di daerah pelindo saya kira begitu ujar ma ruf amin di kawasan plumpang jakarta utara sabtu maret 2023.\"\n",
    "\n",
    "# generate summary\n",
    "input_ids = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors='pt')\n",
    "summary_ids = model.generate(input_ids,\n",
    "            min_length=20,\n",
    "            max_length=80, \n",
    "            num_beams=10,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            use_cache=True,\n",
    "            do_sample = True,\n",
    "            temperature = 0.8,\n",
    "            top_k = 50,\n",
    "            top_p = 0.95)\n",
    "\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proyek\n",
      "tersebut\n",
      "dilakukan\n",
      "di\n",
      "kota\n",
      "jakarta\n",
      "oleh\n",
      "perusahaan\n",
      "yang\n",
      "berbasis\n",
      "di\n",
      "singapura\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "\n",
    "# Load Indonesian spaCy model\n",
    "nlp = Indonesian()\n",
    "\n",
    "# Define a text to tokenize\n",
    "text = \"proyek tersebut dilakukan di kota jakarta oleh perusahaan yang berbasis di singapura\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "processed_text = nlp(text)\n",
    "\n",
    "# Print tokenized text\n",
    "for token in processed_text:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summarizer with Transformer|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "model = EncoderDecoderModel.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your text to summarize\n",
    "text = \"\"\"Perang Dunia II (PD II) adalah konflik global antara Sekutu dan Poros yang terjadi dari 1939 hingga 1945. Dipicu oleh ambisi ekspansionis Jerman Nazi di Eropa dan agresi Jepang di Asia, PD II mengakibatkan perubahan besar dalam politik, ekonomi, dan sosial dunia.\n",
    "\n",
    "Peristiwa penting dalam PD II meliputi invasi Jerman ke Polandia pada 1939 yang memicu deklarasi perang oleh Inggris dan Prancis, serta serangan Jepang ke Pearl Harbor yang memaksa Amerika Serikat masuk dalam perang.\n",
    "\n",
    "Sekutu, yang terdiri dari Amerika Serikat, Inggris, dan Uni Soviet, berhasil menghadapi Poros yang dipimpin oleh Jerman, Italia, dan Jepang. Konflik ini berakhir pada 1945 setelah bom atom dijatuhkan di Hiroshima dan Nagasaki, yang memaksa Jepang menyerah.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konflik global antara sekutu dan poros yang terjadi dari perang dunia ii mengakibatkan perubahan besar dalam politik, ekonomi, dan sosial dunia.\n"
     ]
    }
   ],
   "source": [
    "# generate summary\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "summary_ids = model.generate(input_ids,\n",
    "            min_length=20,\n",
    "            max_length=80, \n",
    "            num_beams=10,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            use_cache=True,\n",
    "            do_sample = True,\n",
    "            temperature = 0.8,\n",
    "            top_k = 50,\n",
    "            top_p = 0.95)\n",
    "\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
