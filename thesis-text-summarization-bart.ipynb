{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import BertTokenizer, BartTokenizer, BertModel, AutoTokenizer, AutoModel, EncoderDecoderModel, BartForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForSeq2Seq, DataCollator\n",
    "\n",
    "# Hiding warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pre = pd.read_csv ('../Dataset/Indonesian News Dataset Preprocessed.csv', encoding='iso-8859-1')\n",
    "post_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total samples\n",
    "total_samples = len(post_pre)\n",
    "\n",
    "# Calculate sizes for each split\n",
    "train_size = int(0.6 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Perform the split\n",
    "train_val, test = post_pre[:train_size + val_size], post_pre[train_size + val_size:]\n",
    "train, val = train_val[:train_size], train_val[train_size:]\n",
    "\n",
    "# Verify the splits (optional)\n",
    "print(f\"Train samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming dataframes into datasets\n",
    "train_ds = Dataset.from_pandas(train)\n",
    "test_ds = Dataset.from_pandas(test)\n",
    "val_ds = Dataset.from_pandas(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "print(train_ds)\n",
    "print('\\n' * 2)\n",
    "print(test_ds)\n",
    "print('\\n' * 2)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading summarization pipeline with the bart-large-cnn model\n",
    "summarizer = pipeline('summarization', model = 'facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/bart-large-cnn' # Model\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint) # Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(checkpoint) # Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model) # Visualizing model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying preprocess_function to the datasets\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True,\n",
    "                               remove_columns=['text', 'summary']) # Removing features\n",
    "\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True,\n",
    "                               remove_columns=['text', 'summary']) # Removing features\n",
    "\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True,\n",
    "                               remove_columns=['text', 'summary']) # Removing features\n",
    "\n",
    "# Printing results\n",
    "print('\\n' * 3)\n",
    "print('Preprocessed Training Dataset:\\n')\n",
    "print(tokenized_train)\n",
    "print('\\n' * 2)\n",
    "print('Preprocessed Test Dataset:\\n')\n",
    "print(tokenized_test)\n",
    "print('\\n' * 2)\n",
    "print('Preprocessed Validation Dataset:\\n')\n",
    "print(tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('rouge') # Loading ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred# Obtaining predictions and true labels\n",
    "    \n",
    "    # Decoding predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label = -100)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    \n",
    "    # Computing rouge score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extracting some results\n",
    "\n",
    "    # Add mean-generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='bart_samsum',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    seed=42,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = 'bart_samsum',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    seed = 42,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model performance on the tokenized test dataset\n",
    "validation = trainer.evaluate(eval_dataset = tokenized_test)\n",
    "print(validation) # Printing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model to a custom directory\n",
    "directory = \"bart_finetuned_samsum_2\"\n",
    "trainer.save_model(directory)\n",
    "\n",
    "# Saving model tokenizer\n",
    "tokenizer.save_pretrained(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in .zip format\n",
    "shutil.make_archive('bart_finetuned_samsum', 'zip', '/kaggle/working/bart_finetuned_samsum')\n",
    "shutil.move('bart_finetuned_samsum.zip', '/kaggle/working/bart_finetuned_samsum.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"perempuan berinisial ag ditahan oleh penyidik ditreskrimum polda metro jaya usai enam jam diperiksa di kasus penganiayaan cristalino david ozora dari hasil pemeriksaan kami selama kurang lebih enam jam kami sekali lagi dengan pertimbangan kenyamanan terhadap anak malam ini kami putuskan dari penyidik kemudian untuk melakukan penangkapan dan dilanjutkan dengan penahan kata direktur reskrimum polda metro jaya kombes hengki haryadi dalam konferensi pers rabu 8/3 hengki turut menjelaskan selama enam jam pemeriksaan itu pihaknya memastikan telah memenuhi seluruh hak ag selaku anak pemenuhan hak anak dari ag kata dia sesuai dengan ketentuan yang diatur dalam sistem peradilan anak dalam hal ini didampingi selain daripada lawyer juga tadi dari tim pembimbing kemasyarakatan bapas jaksel dan juga untuk menjamin pemenuhan hak anak juga didampingi tim dari kemenpppa yang juga merangkap pendamping psikososial dalam rangka dan dalam menjamin pemenuhan terhadap hak hak anak tuturnya sebelumnya ag yang berstatus sebagai pelaku atau anak yang berkonflik dengan hukum resmi ditahan sejak rabu 8/3 kemarin ag bakal ditahan selama tujuh hari ke depan ag dijerat pasal 76c jo pasal 80 uu ppa dan atau pasal 355 ayat jo pasal 56 kuhp subsider pasal 354 ayat jo 56 kuhp subsider 353 ayat jo pasal 56 kuhp mario dijerat dengan pasal 355 kuhp ayat subsider pasal 354 ayat kuhp subsider 535 ayat kuhp subsider 351 ayat kuhp penyidik juga mengenakan mario pasal 76c jo 80 undang-undang perlindungan anak sementara shane dijerat pasal 355 ayat jo pasal 56 kuhp subsider 354 ayat jo 56 kuhp subsider 353 ayat jo 56 kuhp subsider 351 ayat jo 76c undang-undang perlindungan anak.\"\n",
    "summary = \"ag ditahan oleh penyidik polda metro jaya usai diperiksa selama enam jam dalam kasus penganiayaan cristalino david ozora penahanan dilakukan dengan konsiderasi kenyamanan ag sebagai anak ag dijerat pasal 76c jo pasal 80 uu ppa dan atau pasal 355 ayat jo pasal 56 kuhp subsider pasal 354 ayat jo 56 kuhp subsider 353 ayat jo pasal 56 kuhp ag bakal ditahan selama tujuh hari ke depan.\"\n",
    "generated_summary = pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Dialogue:\\n')\n",
    "print(text)\n",
    "print('\\n' * 2)\n",
    "print('Reference Summary:\\n')\n",
    "print(summary)\n",
    "print('\\n' * 2)\n",
    "print('Model-generated Summary:\\n')\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"jakarta cnbc indonesia ceo silicon valley bank svb atau bank terbesar di amerika serikat as yang kolaps greg becker tidak lagi menjadi dewan direksi di federal reserve bank of san francisco juru bicara federal reserve menyebut kepergian becker efektif pada jumat 10/3/2023 pada hari yang sama perusahaan perbankan komersial berbasis california amerika serikat as svb mengalami krisis modal dan bangkrut dalam 48 jam terakhir svb kemudian ditutup oleh regulator selain menutup svb regulator keuangan juga mengambil kendali atas depositonya hal ini diumumkan oleh federal deposit insurance corp fdic melansir reuters kuru bicara itu menolak mengatakan bagaimana becker keluar dari dewan fed san francisco becker menjabat sebagai direktur kelas di san francisco fed salah satu dari tiga eksekutif keuangan yang mewakili bank anggota di distrik fed san francisco setiap bank daerah diawasi oleh dewan yang terdiri dari warga negara selain memiliki tiga direktur yang mewakili bank ada enam direktur lainnya yang menghadirkan perpaduan antara bisnis lokal dan kepentingan masyarakat tiga dari direktur tersebut dipilih oleh dewan gubernur fed di washington sedangkan sisanya dipilih dalam proses lokal sebanyak 12 bank federal reserve regional adalah lembaga kuasi-swasta yang diawasi oleh the fed di washington dewan masing-masing mengawasi bank secara langsung dan memberikan nasihat tentang tata kelola serta kecerdasan ekonomi lokal dewan ini juga memimpin proses pemilihan presiden baru ketika ada lowongan meskipun direktur dari perusahaan yang diatur oleh fed tidak diizinkan untuk berpartisipasi dalam proses tersebut para direktur bank fed telah menjadi sorotan dalam beberapa tahun terakhir karena bank sentral menghadapi kritik terkait direktur bank tidak memiliki keragaman ras dan gender serta terlalu membebani komunitas bisnis dan perbankan the fed telah bekerja untuk memperluas siapa yang melayani dalam peran ini.\"\n",
    "summary2 = \"ceo svb bank terbesar di as greg becker mundur dari dewan direksi federal reserve bank of san francisco hal ini terkait dengan krisis modal dan bangkrutnya svb dalam 48 jam terakhir serta pengambilalihan deposito oleh regulator meskipun dewan gubernur fed di washington yang memimpin pemilihan presiden baru direktur dari perusahaan yang diatur oleh fed tidak diizinkan untuk berpartisipasi dalam proses itu penunjukan direktur bank fed juga telah menghadapi kritik terkait dengan tidak memiliki keragaman ras dan gender.\"\n",
    "generated_summary2 = pipe(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Dialogue:\\n')\n",
    "print(text2)\n",
    "print('\\n' * 2)\n",
    "print('Reference Summary:\\n')\n",
    "print(summary2)\n",
    "print('\\n' * 2)\n",
    "print('Model-generated Summary:\\n')\n",
    "print(generated_summary2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
